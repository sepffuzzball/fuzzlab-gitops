model_list:
  - model_name: gpt-4o
    litellm_params:
      model: openai/gpt-4o
      api_key: os.environ/OPENAI_API_KEY
  - model_name: gpt-4o-mini
    litellm_params:
      model: openai/gpt-4o-mini
      api_key: os.environ/OPENAI_API_KEY
  - model_name: gpt-4.5-preview
    litellm_params:
      model: openai/gpt-4.5-preview
      api_key: os.environ/OPENAI_API_KEY
  - model_name: o1
    litellm_params:
      model: openai/o1
      api_key: os.environ/OPENAI_API_KEY
  - model_name: o1-mini
    litellm_params:
      model: openai/o1-mini
      api_key: os.environ/OPENAI_API_KEY
  - model_name: o3-mini
    litellm_params:
      model: openai/o3-mini
      api_key: os.environ/OPENAI_API_KEY
  - model_name: o3-mini
    litellm_params:
      model: openai/o3-mini
      api_key: os.environ/OPENAI_API_KEY
  - model_name: deepseek-r1-abliterated:70b
    litellm_params:
      model: ollama/huihui_ai/deepseek-r1-abliterated:70b
      api_base: http://10.0.3.205:11434
  - model_name: qwq-abliterated:32b
    litellm_params:
      model: ollama/huihui_ai/qwq-abliterated:latest
      api_base: http://10.0.3.205:11434
  - model_name: midnight-miqu:70b
    litellm_params:
      model: ollama/hf.co/mradermacher/Midnight-Miqu-70B-v1.5-GGUF:Q4_K_M
      api_base: http://10.0.3.205:11434
  - model_name: anubis:70b
    litellm_params:
      model: ollama/hf.co/bartowski/Anubis-70B-v1-GGUF:Q4_K_M
      api_base: http://10.0.3.205:11434
  - model_name: gemma3-abliterated:1b
    litellm_params:
      model: ollama/huihui_ai/gemma3-abliterated:1b
      api_base: http://10.0.3.205:11434
  - model_name: llama3.3-abliterated:70b
    litellm_params:
      model: ollama/huihui_ai/llama3.3-abliterated:70b
      api_base: http://10.0.3.205:11434

litellm_settings:
  cache: true
  cache_params:        # set cache params for redis
    type: redis        # type of cache to initialize

    # Optional - Redis Settings
    namespace: "litellm.caching.caching" # namespace for redis cache
    supported_call_types: ["acompletion", "atext_completion", "aembedding", "atranscription"]
                          # /chat/completions, /completions, /embeddings, /audio/transcriptions
    mode: default_off # if default_off, you need to opt in to caching on a per call basis
    ttl: 600 # ttl for caching

general_settings:
  store_prompts_in_spend_logs: true
  store_model_in_db: true
